{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1**\n",
    "\n",
    "Импортируйте библиотеки pandas и numpy.\n",
    "Загрузите \"Boston House Prices dataset\" из встроенных наборов данных библиотеки sklearn. Создайте датафреймы X и y из этих данных.\n",
    "\n",
    "Разбейте эти датафреймы на тренировочные (X_train, y_train) и тестовые (X_test, y_test) с помощью функции train_test_split так, чтобы размер тестовой выборки\n",
    "составлял 30% от всех данных, при этом аргумент random_state должен быть равен 42.\n",
    "Создайте модель линейной регрессии под названием lr с помощью класса LinearRegression из модуля sklearn.linear_model.\n",
    "\n",
    "Обучите модель на тренировочных данных (используйте все признаки) и сделайте предсказание на тестовых.\n",
    "Вычислите R2 полученных предказаний с помощью r2_score из модуля sklearn.metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = boston['data']\n",
    "target = boston['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = boston['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(data, columns=feature_names)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0  24.0\n",
       "1  21.6\n",
       "2  34.7\n",
       "3  33.4\n",
       "4  36.2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.DataFrame(target)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(354, 13)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7112260057484974"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2**\n",
    "\n",
    "Создайте модель под названием model с помощью RandomForestRegressor из модуля sklearn.ensemble.\n",
    "\n",
    "Сделайте агрумент n_estimators равным 1000,\n",
    "max_depth должен быть равен 12 и random_state сделайте равным 42.\n",
    "Обучите модель на тренировочных данных аналогично тому, как вы обучали модель LinearRegression, но при этом в метод fit вместо датафрейма y_train поставьте y_train.values[:, 0], чтобы получить из датафрейма одномерный массив Numpy,\n",
    "так как для класса RandomForestRegressor в данном методе для аргумента y предпочтительно применение массивов вместо датафрейма.\n",
    "\n",
    "Сделайте предсказание на тестовых данных и посчитайте R2. Сравните с результатом из предыдущего задания.\n",
    "\n",
    "Напишите в комментариях к коду, какая модель в данном случае работает лучше.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 1000\n",
    "max_depth = 12\n",
    "\n",
    "model = RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=12, n_estimators=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=12, n_estimators=1000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=12, n_estimators=1000, random_state=42)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train.values[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_forest = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7112260057484974\n",
      "0.87472606157312\n"
     ]
    }
   ],
   "source": [
    "# В рассматриваемом случае случайный лес показал себя  значительно лучше\n",
    "print(r2_score(y_test, y_pred))\n",
    "print(r2_score(y_test, y_pred_forest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3**\n",
    "\n",
    "Вызовите документацию для класса RandomForestRegressor,\n",
    "найдите информацию об атрибуте feature_importances_.\n",
    "\n",
    "С помощью этого атрибута найдите сумму всех показателей важности,\n",
    "установите, какие два признака показывают наибольшую важность.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestRegressor in module sklearn.ensemble._forest:\n",
      "\n",
      "class RandomForestRegressor(ForestRegressor)\n",
      " |  RandomForestRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest regressor.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of classifying\n",
      " |  decision trees on various sub-samples of the dataset and uses averaging\n",
      " |  to improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"squared_error\", \"absolute_error\", \"poisson\"},             default=\"squared_error\"\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"squared_error\" for the mean squared error, which is equal to\n",
      " |      variance reduction as feature selection criterion, \"absolute_error\"\n",
      " |      for the mean absolute error, and \"poisson\" which uses reduction in\n",
      " |      Poisson deviance to find splits.\n",
      " |      Training using \"absolute_error\" is significantly slower\n",
      " |      than when using \"squared_error\".\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |         Poisson criterion.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Criterion \"mse\" was deprecated in v1.0 and will be removed in\n",
      " |          version 1.2. Use `criterion=\"squared_error\"` which is equivalent.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Criterion \"mae\" was deprecated in v1.0 and will be removed in\n",
      " |          version 1.2. Use `criterion=\"absolute_error\"` which is equivalent.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `round(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None or 1.0, then `max_features=n_features`.\n",
      " |  \n",
      " |      .. note::\n",
      " |          The default of 1.0 is equivalent to bagged trees and more\n",
      " |          randomness can be achieved by setting smaller values, e.g. 0.3.\n",
      " |  \n",
      " |      .. versionchanged:: 1.1\n",
      " |          The default of `max_features` changed from `\"auto\"` to 1.0.\n",
      " |  \n",
      " |      .. deprecated:: 1.1\n",
      " |          The `\"auto\"` option was deprecated in 1.1 and will be removed\n",
      " |          in 1.3.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      " |      Only available if bootstrap=True.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeRegressor\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeRegressor\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      " |          removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |      Prediction computed with out-of-bag estimate on the training set.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
      " |  sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
      " |      tree regressors.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  The default value ``max_features=\"auto\"`` uses ``n_features``\n",
      " |  rather than ``n_features / 3``. The latter was originally suggested in\n",
      " |  [1], whereas the former was more recently justified empirically in [2].\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      " |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  RandomForestRegressor(...)\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-8.32987858]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      ForestRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestRegressor:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      The predicted regression target of an input sample is computed as the\n",
      " |      mean predicted regression targets of the trees in the forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |      \n",
      " |      Number of features when fitting the estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03167574, 0.00154252, 0.00713813, 0.00123624, 0.01426897,\n",
       "       0.40268179, 0.01429864, 0.06397257, 0.00528122, 0.01152493,\n",
       "       0.01808108, 0.01245085, 0.41584732])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# похоже, что max_features тоже важный параметр. Попробуем поиграться\n",
    "score = []\n",
    "feature_values = list(range(1, boston.feature_names.shape[0] + 1))\n",
    "for max_features in feature_values:\n",
    "    model2 = RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators, max_features=max_features, random_state=42)\n",
    "    model2.fit(X_train, y_train.values[:, 0])\n",
    "    score.append(r2_score(y_test, model2.predict(X_test)))\n",
    "plt.plot(feature_values, score, '-ob')\n",
    "plt.plot(score.index(max(score)) + 1, max(score), 'or')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=12, max_features=12, n_estimators=1000,\n",
       "                      random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=12, max_features=12, n_estimators=1000,\n",
       "                      random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=12, max_features=12, n_estimators=1000,\n",
       "                      random_state=42)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators, max_features=12, random_state=42)\n",
    "model2.fit(X_train, y_train.values[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Линейная регрессия:\n",
      "0.7112260057484974\n",
      "Слуйчайный лес с параметрами из задания:\n",
      "0.87472606157312\n",
      "Улучшенный случаный лес:\n",
      "0.8761218900564626\n",
      "\n",
      "Получилось немного улучшить результат\n"
     ]
    }
   ],
   "source": [
    "print(f'Линейная регрессия:\\n{r2_score(y_test, lr.predict(X_test))}')\n",
    "print(f'Слуйчайный лес с параметрами из задания:\\n{r2_score(y_test, model.predict(X_test))}')\n",
    "print(f'Улучшенный случаный лес:\\n{r2_score(y_test, model2.predict(X_test))}')\n",
    "print(f'\\nПолучилось немного улучшить результат')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4**\n",
    "\n",
    "В этом задании мы будем работать с датасетом, с которым мы уже знакомы по домашнему заданию по библиотеке Matplotlib, это датасет Credit Card Fraud Detection.Для этого датасета мы будем решать задачу классификации - будем определять,какие из транзакциции по кредитной карте являются мошенническими.Данный датасет сильно несбалансирован (так как случаи мошенничества относительно редки),так что применение метрики accuracy не принесет пользы и не поможет выбрать лучшую модель.Мы будем вычислять AUC, то есть площадь под кривой ROC.\n",
    "Импортируйте из соответствующих модулей RandomForestClassifier, GridSearchCV и train_test_split.\n",
    "\n",
    "Загрузите датасет creditcard.csv и создайте датафрейм df.\n",
    "С помощью метода value_counts с аргументом normalize=True убедитесь в том, что выборка несбалансирована. Используя метод info, проверьте, все ли столбцы содержат числовые данные и нет ли в них пропусков.Примените следующую настройку, чтобы можно было просматривать все столбцы датафрейма:\n",
    "\n",
    "pd.options.display.max_columns = 100.\n",
    "Просмотрите первые 10 строк датафрейма df.\n",
    "Создайте датафрейм X из датафрейма df, исключив столбец Class.\n",
    "Создайте объект Series под названием y из столбца Class.\n",
    "Разбейте X и y на тренировочный и тестовый наборы данных при помощи функции train_test_split, используя аргументы: test_size=0.3, random_state=100, stratify=y.\n",
    "У вас должны получиться объекты X_train, X_test, y_train и y_test.\n",
    "\n",
    "\n",
    "Просмотрите информацию о их форме.\n",
    "Для поиска по сетке параметров задайте такие параметры:\n",
    "\n",
    "parameters = [{'n_estimators': [10, 15],\n",
    "'max_features': np.arange(3, 5),\n",
    "'max_depth': np.arange(4, 7)}]\n",
    "Создайте модель GridSearchCV со следующими аргументами:\n",
    "estimator=RandomForestClassifier(random_state=100),\n",
    "param_grid=parameters,\n",
    "scoring='roc_auc',\n",
    "cv=3.\n",
    "Обучите модель на тренировочном наборе данных (может занять несколько минут).\n",
    "Просмотрите параметры лучшей модели с помощью атрибута best_params_.\n",
    "Предскажите вероятности классов с помощью полученнной модели и метода predict_proba.\n",
    "Из полученного результата (массив Numpy) выберите столбец с индексом 1 (вероятность класса 1) и запишите в массив y_pred_proba. Из модуля sklearn.metrics импортируйте метрику roc_auc_score.\n",
    "Вычислите AUC на тестовых данных и сравните с результатом,полученным на тренировочных данных, используя в качестве аргументов массивы y_test и y_pred_proba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">163152.0</th>\n",
       "      <th>-1.196037</th>\n",
       "      <th>1.585949</th>\n",
       "      <th>2.883976</th>\n",
       "      <th>3.378471</th>\n",
       "      <th>1.511706</th>\n",
       "      <th>3.717077</th>\n",
       "      <th>0.585362</th>\n",
       "      <th>-0.156001</th>\n",
       "      <th>0.122648</th>\n",
       "      <th>4.217934</th>\n",
       "      <th>1.385525</th>\n",
       "      <th>-0.709405</th>\n",
       "      <th>-0.256168</th>\n",
       "      <th>-1.564352</th>\n",
       "      <th>1.693218</th>\n",
       "      <th>-0.785210</th>\n",
       "      <th>-0.228008</th>\n",
       "      <th>-0.412833</th>\n",
       "      <th>0.234834</th>\n",
       "      <th>1.375790</th>\n",
       "      <th>-0.370294</th>\n",
       "      <th>0.524395</th>\n",
       "      <th>-0.355170</th>\n",
       "      <th>-0.869790</th>\n",
       "      <th>-0.133198</th>\n",
       "      <th>0.327804</th>\n",
       "      <th>-0.035702</th>\n",
       "      <th>-0.858197</th>\n",
       "      <th>7.56</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1.203617</th>\n",
       "      <th>1.574009</th>\n",
       "      <th>2.889277</th>\n",
       "      <th>3.381404</th>\n",
       "      <th>1.538663</th>\n",
       "      <th>3.698747</th>\n",
       "      <th>0.560211</th>\n",
       "      <th>-0.150911</th>\n",
       "      <th>0.124136</th>\n",
       "      <th>4.220998</th>\n",
       "      <th>1.384569</th>\n",
       "      <th>-0.706897</th>\n",
       "      <th>-0.256274</th>\n",
       "      <th>-1.562583</th>\n",
       "      <th>1.692915</th>\n",
       "      <th>-0.787338</th>\n",
       "      <th>-0.226776</th>\n",
       "      <th>-0.412354</th>\n",
       "      <th>0.234322</th>\n",
       "      <th>1.385597</th>\n",
       "      <th>-0.366727</th>\n",
       "      <th>0.522223</th>\n",
       "      <th>-0.357329</th>\n",
       "      <th>-0.870174</th>\n",
       "      <th>-0.134166</th>\n",
       "      <th>0.327019</th>\n",
       "      <th>-0.042648</th>\n",
       "      <th>-0.855262</th>\n",
       "      <th>1.51</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43153.0</th>\n",
       "      <th>-2.086016</th>\n",
       "      <th>2.203265</th>\n",
       "      <th>1.654339</th>\n",
       "      <th>2.941050</th>\n",
       "      <th>-1.683045</th>\n",
       "      <th>0.529728</th>\n",
       "      <th>-1.352162</th>\n",
       "      <th>1.793449</th>\n",
       "      <th>-0.723686</th>\n",
       "      <th>0.600365</th>\n",
       "      <th>-0.982212</th>\n",
       "      <th>-0.551636</th>\n",
       "      <th>-1.337000</th>\n",
       "      <th>0.834403</th>\n",
       "      <th>1.251862</th>\n",
       "      <th>0.033455</th>\n",
       "      <th>1.067978</th>\n",
       "      <th>0.160510</th>\n",
       "      <th>0.213087</th>\n",
       "      <th>0.079002</th>\n",
       "      <th>0.216444</th>\n",
       "      <th>0.567241</th>\n",
       "      <th>-0.035345</th>\n",
       "      <th>0.370201</th>\n",
       "      <th>0.157378</th>\n",
       "      <th>0.440341</th>\n",
       "      <th>0.210230</th>\n",
       "      <th>0.090558</th>\n",
       "      <th>0.76</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170731.0</th>\n",
       "      <th>2.033492</th>\n",
       "      <th>0.766969</th>\n",
       "      <th>-2.107555</th>\n",
       "      <th>3.631952</th>\n",
       "      <th>1.348594</th>\n",
       "      <th>-0.499907</th>\n",
       "      <th>0.945159</th>\n",
       "      <th>-0.286392</th>\n",
       "      <th>-1.370581</th>\n",
       "      <th>1.653073</th>\n",
       "      <th>-1.600434</th>\n",
       "      <th>-1.510901</th>\n",
       "      <th>-2.143280</th>\n",
       "      <th>1.189850</th>\n",
       "      <th>-0.875588</th>\n",
       "      <th>0.175808</th>\n",
       "      <th>-0.419433</th>\n",
       "      <th>-0.464717</th>\n",
       "      <th>-1.414528</th>\n",
       "      <th>-0.430560</th>\n",
       "      <th>0.241894</th>\n",
       "      <th>0.658545</th>\n",
       "      <th>-0.102644</th>\n",
       "      <th>0.580535</th>\n",
       "      <th>0.643637</th>\n",
       "      <th>0.347240</th>\n",
       "      <th>-0.116618</th>\n",
       "      <th>-0.078601</th>\n",
       "      <th>0.76</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68207.0</th>\n",
       "      <th>-13.192671</th>\n",
       "      <th>12.785971</th>\n",
       "      <th>-9.906650</th>\n",
       "      <th>3.320337</th>\n",
       "      <th>-4.801176</th>\n",
       "      <th>5.760059</th>\n",
       "      <th>-18.750889</th>\n",
       "      <th>-37.353443</th>\n",
       "      <th>-0.391540</th>\n",
       "      <th>-5.052502</th>\n",
       "      <th>4.406806</th>\n",
       "      <th>-4.610756</th>\n",
       "      <th>-1.909488</th>\n",
       "      <th>-9.072711</th>\n",
       "      <th>-0.226074</th>\n",
       "      <th>-6.211557</th>\n",
       "      <th>-6.248145</th>\n",
       "      <th>-3.149247</th>\n",
       "      <th>0.051576</th>\n",
       "      <th>-3.493050</th>\n",
       "      <th>27.202839</th>\n",
       "      <th>-8.887017</th>\n",
       "      <th>5.303607</th>\n",
       "      <th>-0.639435</th>\n",
       "      <th>0.263203</th>\n",
       "      <th>-0.108877</th>\n",
       "      <th>1.269566</th>\n",
       "      <th>0.939407</th>\n",
       "      <th>1.00</th>\n",
       "      <th>1</th>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">65149.0</th>\n",
       "      <th>-0.608037</th>\n",
       "      <th>0.277482</th>\n",
       "      <th>2.333740</th>\n",
       "      <th>0.713876</th>\n",
       "      <th>-0.686327</th>\n",
       "      <th>0.424502</th>\n",
       "      <th>0.158410</th>\n",
       "      <th>0.277078</th>\n",
       "      <th>0.005665</th>\n",
       "      <th>-0.574444</th>\n",
       "      <th>-0.383596</th>\n",
       "      <th>0.063757</th>\n",
       "      <th>0.435809</th>\n",
       "      <th>-0.294166</th>\n",
       "      <th>1.561564</th>\n",
       "      <th>0.430549</th>\n",
       "      <th>-0.512260</th>\n",
       "      <th>0.321857</th>\n",
       "      <th>-1.089111</th>\n",
       "      <th>0.192164</th>\n",
       "      <th>0.425425</th>\n",
       "      <th>1.077523</th>\n",
       "      <th>0.095700</th>\n",
       "      <th>0.080007</th>\n",
       "      <th>-0.087784</th>\n",
       "      <th>-0.253436</th>\n",
       "      <th>0.077868</th>\n",
       "      <th>0.055774</th>\n",
       "      <th>115.98</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.890428</th>\n",
       "      <th>-0.914533</th>\n",
       "      <th>0.916273</th>\n",
       "      <th>0.533497</th>\n",
       "      <th>-1.417793</th>\n",
       "      <th>-0.283902</th>\n",
       "      <th>-0.520284</th>\n",
       "      <th>0.002223</th>\n",
       "      <th>-1.050330</th>\n",
       "      <th>0.827726</th>\n",
       "      <th>1.336306</th>\n",
       "      <th>0.961705</th>\n",
       "      <th>0.778165</th>\n",
       "      <th>0.101997</th>\n",
       "      <th>0.352339</th>\n",
       "      <th>-0.892199</th>\n",
       "      <th>-0.538873</th>\n",
       "      <th>1.792922</th>\n",
       "      <th>-1.092627</th>\n",
       "      <th>-0.119284</th>\n",
       "      <th>-0.239564</th>\n",
       "      <th>-0.634749</th>\n",
       "      <th>-0.018377</th>\n",
       "      <th>0.482486</th>\n",
       "      <th>0.102384</th>\n",
       "      <th>-0.559266</th>\n",
       "      <th>0.040121</th>\n",
       "      <th>0.067240</th>\n",
       "      <th>192.05</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">65150.0</th>\n",
       "      <th>-0.819167</th>\n",
       "      <th>1.289630</th>\n",
       "      <th>1.155617</th>\n",
       "      <th>-0.356589</th>\n",
       "      <th>0.742668</th>\n",
       "      <th>-1.179886</th>\n",
       "      <th>1.114827</th>\n",
       "      <th>-0.105033</th>\n",
       "      <th>-1.169136</th>\n",
       "      <th>-1.218791</th>\n",
       "      <th>1.841286</th>\n",
       "      <th>0.558376</th>\n",
       "      <th>0.081792</th>\n",
       "      <th>-1.028918</th>\n",
       "      <th>-0.400824</th>\n",
       "      <th>0.722658</th>\n",
       "      <th>0.402985</th>\n",
       "      <th>0.613975</th>\n",
       "      <th>-0.605494</th>\n",
       "      <th>-0.014715</th>\n",
       "      <th>-0.011025</th>\n",
       "      <th>-0.125263</th>\n",
       "      <th>-0.385443</th>\n",
       "      <th>0.449483</th>\n",
       "      <th>0.536560</th>\n",
       "      <th>0.252429</th>\n",
       "      <th>-0.020876</th>\n",
       "      <th>0.072608</th>\n",
       "      <th>0.76</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0.283939</th>\n",
       "      <th>1.355339</th>\n",
       "      <th>0.553398</th>\n",
       "      <th>0.255501</th>\n",
       "      <th>0.561040</th>\n",
       "      <th>-1.338352</th>\n",
       "      <th>1.056880</th>\n",
       "      <th>-0.229176</th>\n",
       "      <th>-0.738105</th>\n",
       "      <th>-1.157676</th>\n",
       "      <th>0.000759</th>\n",
       "      <th>-0.543236</th>\n",
       "      <th>-0.313497</th>\n",
       "      <th>-1.370815</th>\n",
       "      <th>0.770587</th>\n",
       "      <th>0.452886</th>\n",
       "      <th>1.064176</th>\n",
       "      <th>0.458320</th>\n",
       "      <th>-0.199074</th>\n",
       "      <th>-0.019922</th>\n",
       "      <th>-0.076192</th>\n",
       "      <th>-0.211969</th>\n",
       "      <th>-0.256209</th>\n",
       "      <th>0.259185</th>\n",
       "      <th>0.096589</th>\n",
       "      <th>0.327896</th>\n",
       "      <th>0.021232</th>\n",
       "      <th>0.083294</th>\n",
       "      <th>0.76</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172792.0</th>\n",
       "      <th>-0.533413</th>\n",
       "      <th>-0.189733</th>\n",
       "      <th>0.703337</th>\n",
       "      <th>-0.506271</th>\n",
       "      <th>-0.012546</th>\n",
       "      <th>-0.649617</th>\n",
       "      <th>1.577006</th>\n",
       "      <th>-0.414650</th>\n",
       "      <th>0.486180</th>\n",
       "      <th>-0.915427</th>\n",
       "      <th>-1.040458</th>\n",
       "      <th>-0.031513</th>\n",
       "      <th>-0.188093</th>\n",
       "      <th>-0.084316</th>\n",
       "      <th>0.041333</th>\n",
       "      <th>-0.302620</th>\n",
       "      <th>-0.660377</th>\n",
       "      <th>0.167430</th>\n",
       "      <th>-0.256117</th>\n",
       "      <th>0.382948</th>\n",
       "      <th>0.261057</th>\n",
       "      <th>0.643078</th>\n",
       "      <th>0.376777</th>\n",
       "      <th>0.008797</th>\n",
       "      <th>-0.473649</th>\n",
       "      <th>-0.818267</th>\n",
       "      <th>-0.002415</th>\n",
       "      <th>0.013649</th>\n",
       "      <th>217.00</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283726 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                           0\n",
       "Time     V1         V2         V3        V4        V5        V6        V7         V8         V9        V10       V11       V12       V13       V14       V15       V16       V17       V18       V19       V20       V21        V22       V23       V24       V25       V26       V27       V28       Amount Class          \n",
       "163152.0 -1.196037   1.585949   2.883976  3.378471  1.511706  3.717077  0.585362  -0.156001   0.122648  4.217934  1.385525 -0.709405 -0.256168 -1.564352  1.693218 -0.785210 -0.228008 -0.412833  0.234834  1.375790 -0.370294   0.524395 -0.355170 -0.869790 -0.133198  0.327804 -0.035702 -0.858197 7.56   0      0.000063\n",
       "         -1.203617   1.574009   2.889277  3.381404  1.538663  3.698747  0.560211  -0.150911   0.124136  4.220998  1.384569 -0.706897 -0.256274 -1.562583  1.692915 -0.787338 -0.226776 -0.412354  0.234322  1.385597 -0.366727   0.522223 -0.357329 -0.870174 -0.134166  0.327019 -0.042648 -0.855262 1.51   0      0.000063\n",
       "43153.0  -2.086016   2.203265   1.654339  2.941050 -1.683045  0.529728 -1.352162   1.793449  -0.723686  0.600365 -0.982212 -0.551636 -1.337000  0.834403  1.251862  0.033455  1.067978  0.160510  0.213087  0.079002  0.216444   0.567241 -0.035345  0.370201  0.157378  0.440341  0.210230  0.090558 0.76   0      0.000032\n",
       "170731.0  2.033492   0.766969  -2.107555  3.631952  1.348594 -0.499907  0.945159  -0.286392  -1.370581  1.653073 -1.600434 -1.510901 -2.143280  1.189850 -0.875588  0.175808 -0.419433 -0.464717 -1.414528 -0.430560  0.241894   0.658545 -0.102644  0.580535  0.643637  0.347240 -0.116618 -0.078601 0.76   0      0.000032\n",
       "68207.0  -13.192671  12.785971 -9.906650  3.320337 -4.801176  5.760059 -18.750889 -37.353443 -0.391540 -5.052502  4.406806 -4.610756 -1.909488 -9.072711 -0.226074 -6.211557 -6.248145 -3.149247  0.051576 -3.493050  27.202839 -8.887017  5.303607 -0.639435  0.263203 -0.108877  1.269566  0.939407 1.00   1      0.000021\n",
       "...                                                                                                                                                                                                                                                                                                                      ...\n",
       "65149.0  -0.608037   0.277482   2.333740  0.713876 -0.686327  0.424502  0.158410   0.277078   0.005665 -0.574444 -0.383596  0.063757  0.435809 -0.294166  1.561564  0.430549 -0.512260  0.321857 -1.089111  0.192164  0.425425   1.077523  0.095700  0.080007 -0.087784 -0.253436  0.077868  0.055774 115.98 0      0.000004\n",
       "          0.890428  -0.914533   0.916273  0.533497 -1.417793 -0.283902 -0.520284   0.002223  -1.050330  0.827726  1.336306  0.961705  0.778165  0.101997  0.352339 -0.892199 -0.538873  1.792922 -1.092627 -0.119284 -0.239564  -0.634749 -0.018377  0.482486  0.102384 -0.559266  0.040121  0.067240 192.05 0      0.000004\n",
       "65150.0  -0.819167   1.289630   1.155617 -0.356589  0.742668 -1.179886  1.114827  -0.105033  -1.169136 -1.218791  1.841286  0.558376  0.081792 -1.028918 -0.400824  0.722658  0.402985  0.613975 -0.605494 -0.014715 -0.011025  -0.125263 -0.385443  0.449483  0.536560  0.252429 -0.020876  0.072608 0.76   0      0.000004\n",
       "         -0.283939   1.355339   0.553398  0.255501  0.561040 -1.338352  1.056880  -0.229176  -0.738105 -1.157676  0.000759 -0.543236 -0.313497 -1.370815  0.770587  0.452886  1.064176  0.458320 -0.199074 -0.019922 -0.076192  -0.211969 -0.256209  0.259185  0.096589  0.327896  0.021232  0.083294 0.76   0      0.000004\n",
       "172792.0 -0.533413  -0.189733   0.703337 -0.506271 -0.012546 -0.649617  1.577006  -0.414650   0.486180 -0.915427 -1.040458 -0.031513 -0.188093 -0.084316  0.041333 -0.302620 -0.660377  0.167430 -0.256117  0.382948  0.261057   0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649 217.00 0      0.000004\n",
       "\n",
       "[283726 rows x 1 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame(df.value_counts(normalize=True))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7ef9e01e40>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df.value_counts(normalize=True).values, '-ob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>-0.371407</td>\n",
       "      <td>1.341262</td>\n",
       "      <td>0.359894</td>\n",
       "      <td>-0.358091</td>\n",
       "      <td>-0.137134</td>\n",
       "      <td>0.517617</td>\n",
       "      <td>0.401726</td>\n",
       "      <td>-0.058133</td>\n",
       "      <td>0.068653</td>\n",
       "      <td>-0.033194</td>\n",
       "      <td>0.084968</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>-1.416907</td>\n",
       "      <td>-0.153826</td>\n",
       "      <td>-0.751063</td>\n",
       "      <td>0.167372</td>\n",
       "      <td>0.050144</td>\n",
       "      <td>-0.443587</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>-0.611987</td>\n",
       "      <td>-0.045575</td>\n",
       "      <td>-0.219633</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>1.249376</td>\n",
       "      <td>-0.619468</td>\n",
       "      <td>0.291474</td>\n",
       "      <td>1.757964</td>\n",
       "      <td>-1.323865</td>\n",
       "      <td>0.686133</td>\n",
       "      <td>-0.076127</td>\n",
       "      <td>-1.222127</td>\n",
       "      <td>-0.358222</td>\n",
       "      <td>0.324505</td>\n",
       "      <td>-0.156742</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>-0.705117</td>\n",
       "      <td>-0.110452</td>\n",
       "      <td>-0.286254</td>\n",
       "      <td>0.074355</td>\n",
       "      <td>-0.328783</td>\n",
       "      <td>-0.210077</td>\n",
       "      <td>-0.499768</td>\n",
       "      <td>0.118765</td>\n",
       "      <td>0.570328</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>1.017614</td>\n",
       "      <td>0.836390</td>\n",
       "      <td>1.006844</td>\n",
       "      <td>-0.443523</td>\n",
       "      <td>0.150219</td>\n",
       "      <td>0.739453</td>\n",
       "      <td>-0.540980</td>\n",
       "      <td>0.476677</td>\n",
       "      <td>0.451773</td>\n",
       "      <td>0.203711</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "5  0.260314 -0.568671 -0.371407  1.341262  0.359894 -0.358091 -0.137134   \n",
       "6  0.081213  0.464960 -0.099254 -1.416907 -0.153826 -0.751063  0.167372   \n",
       "7 -3.807864  0.615375  1.249376 -0.619468  0.291474  1.757964 -1.323865   \n",
       "8  0.851084 -0.392048 -0.410430 -0.705117 -0.110452 -0.286254  0.074355   \n",
       "9  0.069539 -0.736727 -0.366846  1.017614  0.836390  1.006844 -0.443523   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "5  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254   \n",
       "6  0.050144 -0.443587  0.002821 -0.611987 -0.045575 -0.219633 -0.167716   \n",
       "7  0.686133 -0.076127 -1.222127 -0.358222  0.324505 -0.156742  1.943465   \n",
       "8 -0.328783 -0.210077 -0.499768  0.118765  0.570328  0.052736 -0.073425   \n",
       "9  0.150219  0.739453 -0.540980  0.476677  0.451773  0.203711 -0.246914   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "5 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   \n",
       "6 -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   \n",
       "7 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   \n",
       "8 -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n",
       "9 -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  \n",
       "5    3.67      0  \n",
       "6    4.99      0  \n",
       "7   40.80      0  \n",
       "8   93.20      0  \n",
       "9    3.68      0  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>-1.593105</td>\n",
       "      <td>2.711941</td>\n",
       "      <td>-0.689256</td>\n",
       "      <td>4.626942</td>\n",
       "      <td>-0.924459</td>\n",
       "      <td>1.107641</td>\n",
       "      <td>1.991691</td>\n",
       "      <td>0.510632</td>\n",
       "      <td>-0.682920</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>-0.150189</td>\n",
       "      <td>0.915802</td>\n",
       "      <td>1.214756</td>\n",
       "      <td>-0.675143</td>\n",
       "      <td>1.164931</td>\n",
       "      <td>-0.711757</td>\n",
       "      <td>-0.025693</td>\n",
       "      <td>-1.221179</td>\n",
       "      <td>-1.545556</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>0.411614</td>\n",
       "      <td>0.063119</td>\n",
       "      <td>-0.183699</td>\n",
       "      <td>-0.510602</td>\n",
       "      <td>1.329284</td>\n",
       "      <td>0.140716</td>\n",
       "      <td>0.313502</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>-0.577252</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>-1.933849</td>\n",
       "      <td>-0.962886</td>\n",
       "      <td>-1.042082</td>\n",
       "      <td>0.449624</td>\n",
       "      <td>1.962563</td>\n",
       "      <td>-0.608577</td>\n",
       "      <td>0.509928</td>\n",
       "      <td>1.113981</td>\n",
       "      <td>2.897849</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>-1.040458</td>\n",
       "      <td>-0.031513</td>\n",
       "      <td>-0.188093</td>\n",
       "      <td>-0.084316</td>\n",
       "      <td>0.041333</td>\n",
       "      <td>-0.302620</td>\n",
       "      <td>-0.660377</td>\n",
       "      <td>0.167430</td>\n",
       "      <td>-0.256117</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9       V10       V11       V12  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  0.090794 -0.551600 -0.617801   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425 -0.166974  1.612727  1.065235   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  0.207643  0.624501  0.066084   \n",
       "3       1.247203  0.237609  0.377436 -1.387024 -0.054952 -0.226487  0.178228   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  0.753074 -0.822843  0.538196   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  4.356170 -1.593105  2.711941   \n",
       "284803  1.058415  0.024330  0.294869  0.584800 -0.975926 -0.150189  0.915802   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454 -0.484782  0.411614  0.063119   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087 -0.399126 -1.933849 -0.962886   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180 -0.915427 -1.040458 -0.031513   \n",
       "\n",
       "             V13       V14       V15       V16       V17       V18       V19  \\\n",
       "0      -0.991390 -0.311169  1.468177 -0.470401  0.207971  0.025791  0.403993   \n",
       "1       0.489095 -0.143772  0.635558  0.463917 -0.114805 -0.183361 -0.145783   \n",
       "2       0.717293 -0.165946  2.345865 -2.890083  1.109969 -0.121359 -2.261857   \n",
       "3       0.507757 -0.287924 -0.631418 -1.059647 -0.684093  1.965775 -1.232622   \n",
       "4       1.345852 -1.119670  0.175121 -0.451449 -0.237033 -0.038195  0.803487   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802 -0.689256  4.626942 -0.924459  1.107641  1.991691  0.510632 -0.682920   \n",
       "284803  1.214756 -0.675143  1.164931 -0.711757 -0.025693 -1.221179 -1.545556   \n",
       "284804 -0.183699 -0.510602  1.329284  0.140716  0.313502  0.395652 -0.577252   \n",
       "284805 -1.042082  0.449624  1.962563 -0.608577  0.509928  1.113981  2.897849   \n",
       "284806 -0.188093 -0.084316  0.041333 -0.302620 -0.660377  0.167430 -0.256117   \n",
       "\n",
       "             V20       V21       V22       V23       V24       V25       V26  \\\n",
       "0       0.251412 -0.018307  0.277838 -0.110474  0.066928  0.128539 -0.189115   \n",
       "1      -0.069083 -0.225775 -0.638672  0.101288 -0.339846  0.167170  0.125895   \n",
       "2       0.524980  0.247998  0.771679  0.909412 -0.689281 -0.327642 -0.139097   \n",
       "3      -0.208038 -0.108300  0.005274 -0.190321 -1.175575  0.647376 -0.221929   \n",
       "4       0.408542 -0.009431  0.798278 -0.137458  0.141267 -0.206010  0.502292   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  1.475829  0.213454  0.111864  1.014480 -0.509348  1.436807  0.250034   \n",
       "284803  0.059616  0.214205  0.924384  0.012463 -1.016226 -0.606624 -0.395255   \n",
       "284804  0.001396  0.232045  0.578229 -0.037501  0.640134  0.265745 -0.087371   \n",
       "284805  0.127434  0.265245  0.800049 -0.163298  0.123205 -0.569159  0.546668   \n",
       "284806  0.382948  0.261057  0.643078  0.376777  0.008797 -0.473649 -0.818267   \n",
       "\n",
       "             V27       V28  Amount  \n",
       "0       0.133558 -0.021053  149.62  \n",
       "1      -0.008983  0.014724    2.69  \n",
       "2      -0.055353 -0.059752  378.66  \n",
       "3       0.062723  0.061458  123.50  \n",
       "4       0.219422  0.215153   69.99  \n",
       "...          ...       ...     ...  \n",
       "284802  0.943651  0.823731    0.77  \n",
       "284803  0.068472 -0.053527   24.79  \n",
       "284804  0.004455 -0.026561   67.88  \n",
       "284805  0.108821  0.104533   10.00  \n",
       "284806 -0.002415  0.013649  217.00  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "284802    0\n",
       "284803    0\n",
       "284804    0\n",
       "284805    0\n",
       "284806    0\n",
       "Name: Class, Length: 284807, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.iloc[:, -1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (199364, 30),         X_test.shape = (85443, 30),         y_train.shape = (199364,),         y_test.shape = (85443,)\n"
     ]
    }
   ],
   "source": [
    "print(f'{X_train.shape = }, \\\n",
    "        {X_test.shape = }, \\\n",
    "        {y_train.shape = }, \\\n",
    "        {y_test.shape = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'n_estimators': [10, 15],\n",
    "               'max_features': np.arange(3, 5),\n",
    "               'max_depth': np.arange(4, 7)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gs = GridSearchCV(estimator=RandomForestClassifier(random_state=100),\n",
    "                         param_grid=parameters,\n",
    "                         scoring='roc_auc',\n",
    "                         cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=100),\n",
       "             param_grid=[{&#x27;max_depth&#x27;: array([4, 5, 6]),\n",
       "                          &#x27;max_features&#x27;: array([3, 4]),\n",
       "                          &#x27;n_estimators&#x27;: [10, 15]}],\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=100),\n",
       "             param_grid=[{&#x27;max_depth&#x27;: array([4, 5, 6]),\n",
       "                          &#x27;max_features&#x27;: array([3, 4]),\n",
       "                          &#x27;n_estimators&#x27;: [10, 15]}],\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=100)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=100)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=100),\n",
       "             param_grid=[{'max_depth': array([4, 5, 6]),\n",
       "                          'max_features': array([3, 4]),\n",
       "                          'n_estimators': [10, 15]}],\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6, 'max_features': 3, 'n_estimators': 15}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00092917, 0.00029521, 0.00028215, ..., 0.00028215, 0.0006822 ,\n",
       "       0.01246098])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = model_gs.predict_proba(X_test)[:, 1]\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9462664156037156"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Дополнительные задания:\n",
    "\n",
    "1). Загрузите датасет Wine из встроенных датасетов sklearn.datasets с помощью функции load_wine в переменную data.\n",
    "\n",
    "2). Полученный датасет не является датафреймом. Это структура данных, имеющая ключи аналогично словарю. Просмотрите тип данных этой структуры данных и создайте список data_keys, содержащий ее ключи.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = load_wine()\n",
    "type(wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_keys = list(wine.keys())\n",
    "data_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3). Просмотрите данные, описание и названия признаков в датасете. Описание нужно вывести в виде привычного, аккуратно оформленного текста, без обозначений переноса строки, но с самими переносами и т.д.4). Сколько классов содержит целевая переменная датасета? Выве\n",
    "дите названия классов.\n",
    "\n",
    "3 класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['class_0', 'class_1', 'class_2'], dtype='<U7')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(wine['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5). На основе данных датасета (они содержатся в двумерном массиве Numpy) и названий признаков создайте датафрейм под названием X.\n",
    "\n",
    "6). Выясните размер датафрейма X и установите, имеются ли в нем пропущенные значения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  \n",
       "0                            3.92   1065.0  \n",
       "1                            3.40   1050.0  \n",
       "2                            3.17   1185.0  \n",
       "3                            3.45   1480.0  \n",
       "4                            2.93    735.0  \n",
       "..                            ...      ...  \n",
       "173                          1.74    740.0  \n",
       "174                          1.56    750.0  \n",
       "175                          1.56    835.0  \n",
       "176                          1.62    840.0  \n",
       "177                          1.60    560.0  \n",
       "\n",
       "[178 rows x 13 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 13 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       178 non-null    float64\n",
      " 1   malic_acid                    178 non-null    float64\n",
      " 2   ash                           178 non-null    float64\n",
      " 3   alcalinity_of_ash             178 non-null    float64\n",
      " 4   magnesium                     178 non-null    float64\n",
      " 5   total_phenols                 178 non-null    float64\n",
      " 6   flavanoids                    178 non-null    float64\n",
      " 7   nonflavanoid_phenols          178 non-null    float64\n",
      " 8   proanthocyanins               178 non-null    float64\n",
      " 9   color_intensity               178 non-null    float64\n",
      " 10  hue                           178 non-null    float64\n",
      " 11  od280/od315_of_diluted_wines  178 non-null    float64\n",
      " 12  proline                       178 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 18.2 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7). Добавьте в датафрейм поле с классами вин в виде чисел, имеющих тип данных numpy.int64. Название поля - 'target'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "0                            3.92   1065.0       0  \n",
       "1                            3.40   1050.0       0  \n",
       "2                            3.17   1185.0       0  \n",
       "3                            3.45   1480.0       0  \n",
       "4                            2.93    735.0       0  \n",
       "..                            ...      ...     ...  \n",
       "173                          1.74    740.0       2  \n",
       "174                          1.56    750.0       2  \n",
       "175                          1.56    835.0       2  \n",
       "176                          1.62    840.0       2  \n",
       "177                          1.60    560.0       2  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['target'] = wine.target\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8). Постройте матрицу корреляций для всех полей X. Дайте полученному датафрейму название X_corr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alcohol</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094397</td>\n",
       "      <td>0.211545</td>\n",
       "      <td>-0.310235</td>\n",
       "      <td>0.270798</td>\n",
       "      <td>0.289101</td>\n",
       "      <td>0.236815</td>\n",
       "      <td>-0.155929</td>\n",
       "      <td>0.136698</td>\n",
       "      <td>0.546364</td>\n",
       "      <td>-0.071747</td>\n",
       "      <td>0.072343</td>\n",
       "      <td>0.643720</td>\n",
       "      <td>-0.328222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>malic_acid</th>\n",
       "      <td>0.094397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164045</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>-0.054575</td>\n",
       "      <td>-0.335167</td>\n",
       "      <td>-0.411007</td>\n",
       "      <td>0.292977</td>\n",
       "      <td>-0.220746</td>\n",
       "      <td>0.248985</td>\n",
       "      <td>-0.561296</td>\n",
       "      <td>-0.368710</td>\n",
       "      <td>-0.192011</td>\n",
       "      <td>0.437776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ash</th>\n",
       "      <td>0.211545</td>\n",
       "      <td>0.164045</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.443367</td>\n",
       "      <td>0.286587</td>\n",
       "      <td>0.128980</td>\n",
       "      <td>0.115077</td>\n",
       "      <td>0.186230</td>\n",
       "      <td>0.009652</td>\n",
       "      <td>0.258887</td>\n",
       "      <td>-0.074667</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.223626</td>\n",
       "      <td>-0.049643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <td>-0.310235</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>0.443367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>-0.321113</td>\n",
       "      <td>-0.351370</td>\n",
       "      <td>0.361922</td>\n",
       "      <td>-0.197327</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>-0.273955</td>\n",
       "      <td>-0.276769</td>\n",
       "      <td>-0.440597</td>\n",
       "      <td>0.517859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magnesium</th>\n",
       "      <td>0.270798</td>\n",
       "      <td>-0.054575</td>\n",
       "      <td>0.286587</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.214401</td>\n",
       "      <td>0.195784</td>\n",
       "      <td>-0.256294</td>\n",
       "      <td>0.236441</td>\n",
       "      <td>0.199950</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>0.066004</td>\n",
       "      <td>0.393351</td>\n",
       "      <td>-0.209179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_phenols</th>\n",
       "      <td>0.289101</td>\n",
       "      <td>-0.335167</td>\n",
       "      <td>0.128980</td>\n",
       "      <td>-0.321113</td>\n",
       "      <td>0.214401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.864564</td>\n",
       "      <td>-0.449935</td>\n",
       "      <td>0.612413</td>\n",
       "      <td>-0.055136</td>\n",
       "      <td>0.433681</td>\n",
       "      <td>0.699949</td>\n",
       "      <td>0.498115</td>\n",
       "      <td>-0.719163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flavanoids</th>\n",
       "      <td>0.236815</td>\n",
       "      <td>-0.411007</td>\n",
       "      <td>0.115077</td>\n",
       "      <td>-0.351370</td>\n",
       "      <td>0.195784</td>\n",
       "      <td>0.864564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.537900</td>\n",
       "      <td>0.652692</td>\n",
       "      <td>-0.172379</td>\n",
       "      <td>0.543479</td>\n",
       "      <td>0.787194</td>\n",
       "      <td>0.494193</td>\n",
       "      <td>-0.847498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <td>-0.155929</td>\n",
       "      <td>0.292977</td>\n",
       "      <td>0.186230</td>\n",
       "      <td>0.361922</td>\n",
       "      <td>-0.256294</td>\n",
       "      <td>-0.449935</td>\n",
       "      <td>-0.537900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.365845</td>\n",
       "      <td>0.139057</td>\n",
       "      <td>-0.262640</td>\n",
       "      <td>-0.503270</td>\n",
       "      <td>-0.311385</td>\n",
       "      <td>0.489109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proanthocyanins</th>\n",
       "      <td>0.136698</td>\n",
       "      <td>-0.220746</td>\n",
       "      <td>0.009652</td>\n",
       "      <td>-0.197327</td>\n",
       "      <td>0.236441</td>\n",
       "      <td>0.612413</td>\n",
       "      <td>0.652692</td>\n",
       "      <td>-0.365845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025250</td>\n",
       "      <td>0.295544</td>\n",
       "      <td>0.519067</td>\n",
       "      <td>0.330417</td>\n",
       "      <td>-0.499130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_intensity</th>\n",
       "      <td>0.546364</td>\n",
       "      <td>0.248985</td>\n",
       "      <td>0.258887</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>0.199950</td>\n",
       "      <td>-0.055136</td>\n",
       "      <td>-0.172379</td>\n",
       "      <td>0.139057</td>\n",
       "      <td>-0.025250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.521813</td>\n",
       "      <td>-0.428815</td>\n",
       "      <td>0.316100</td>\n",
       "      <td>0.265668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hue</th>\n",
       "      <td>-0.071747</td>\n",
       "      <td>-0.561296</td>\n",
       "      <td>-0.074667</td>\n",
       "      <td>-0.273955</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>0.433681</td>\n",
       "      <td>0.543479</td>\n",
       "      <td>-0.262640</td>\n",
       "      <td>0.295544</td>\n",
       "      <td>-0.521813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.565468</td>\n",
       "      <td>0.236183</td>\n",
       "      <td>-0.617369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <td>0.072343</td>\n",
       "      <td>-0.368710</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>-0.276769</td>\n",
       "      <td>0.066004</td>\n",
       "      <td>0.699949</td>\n",
       "      <td>0.787194</td>\n",
       "      <td>-0.503270</td>\n",
       "      <td>0.519067</td>\n",
       "      <td>-0.428815</td>\n",
       "      <td>0.565468</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.312761</td>\n",
       "      <td>-0.788230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proline</th>\n",
       "      <td>0.643720</td>\n",
       "      <td>-0.192011</td>\n",
       "      <td>0.223626</td>\n",
       "      <td>-0.440597</td>\n",
       "      <td>0.393351</td>\n",
       "      <td>0.498115</td>\n",
       "      <td>0.494193</td>\n",
       "      <td>-0.311385</td>\n",
       "      <td>0.330417</td>\n",
       "      <td>0.316100</td>\n",
       "      <td>0.236183</td>\n",
       "      <td>0.312761</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.633717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>-0.328222</td>\n",
       "      <td>0.437776</td>\n",
       "      <td>-0.049643</td>\n",
       "      <td>0.517859</td>\n",
       "      <td>-0.209179</td>\n",
       "      <td>-0.719163</td>\n",
       "      <td>-0.847498</td>\n",
       "      <td>0.489109</td>\n",
       "      <td>-0.499130</td>\n",
       "      <td>0.265668</td>\n",
       "      <td>-0.617369</td>\n",
       "      <td>-0.788230</td>\n",
       "      <td>-0.633717</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               alcohol  malic_acid       ash  \\\n",
       "alcohol                       1.000000    0.094397  0.211545   \n",
       "malic_acid                    0.094397    1.000000  0.164045   \n",
       "ash                           0.211545    0.164045  1.000000   \n",
       "alcalinity_of_ash            -0.310235    0.288500  0.443367   \n",
       "magnesium                     0.270798   -0.054575  0.286587   \n",
       "total_phenols                 0.289101   -0.335167  0.128980   \n",
       "flavanoids                    0.236815   -0.411007  0.115077   \n",
       "nonflavanoid_phenols         -0.155929    0.292977  0.186230   \n",
       "proanthocyanins               0.136698   -0.220746  0.009652   \n",
       "color_intensity               0.546364    0.248985  0.258887   \n",
       "hue                          -0.071747   -0.561296 -0.074667   \n",
       "od280/od315_of_diluted_wines  0.072343   -0.368710  0.003911   \n",
       "proline                       0.643720   -0.192011  0.223626   \n",
       "target                       -0.328222    0.437776 -0.049643   \n",
       "\n",
       "                              alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "alcohol                               -0.310235   0.270798       0.289101   \n",
       "malic_acid                             0.288500  -0.054575      -0.335167   \n",
       "ash                                    0.443367   0.286587       0.128980   \n",
       "alcalinity_of_ash                      1.000000  -0.083333      -0.321113   \n",
       "magnesium                             -0.083333   1.000000       0.214401   \n",
       "total_phenols                         -0.321113   0.214401       1.000000   \n",
       "flavanoids                            -0.351370   0.195784       0.864564   \n",
       "nonflavanoid_phenols                   0.361922  -0.256294      -0.449935   \n",
       "proanthocyanins                       -0.197327   0.236441       0.612413   \n",
       "color_intensity                        0.018732   0.199950      -0.055136   \n",
       "hue                                   -0.273955   0.055398       0.433681   \n",
       "od280/od315_of_diluted_wines          -0.276769   0.066004       0.699949   \n",
       "proline                               -0.440597   0.393351       0.498115   \n",
       "target                                 0.517859  -0.209179      -0.719163   \n",
       "\n",
       "                              flavanoids  nonflavanoid_phenols  \\\n",
       "alcohol                         0.236815             -0.155929   \n",
       "malic_acid                     -0.411007              0.292977   \n",
       "ash                             0.115077              0.186230   \n",
       "alcalinity_of_ash              -0.351370              0.361922   \n",
       "magnesium                       0.195784             -0.256294   \n",
       "total_phenols                   0.864564             -0.449935   \n",
       "flavanoids                      1.000000             -0.537900   \n",
       "nonflavanoid_phenols           -0.537900              1.000000   \n",
       "proanthocyanins                 0.652692             -0.365845   \n",
       "color_intensity                -0.172379              0.139057   \n",
       "hue                             0.543479             -0.262640   \n",
       "od280/od315_of_diluted_wines    0.787194             -0.503270   \n",
       "proline                         0.494193             -0.311385   \n",
       "target                         -0.847498              0.489109   \n",
       "\n",
       "                              proanthocyanins  color_intensity       hue  \\\n",
       "alcohol                              0.136698         0.546364 -0.071747   \n",
       "malic_acid                          -0.220746         0.248985 -0.561296   \n",
       "ash                                  0.009652         0.258887 -0.074667   \n",
       "alcalinity_of_ash                   -0.197327         0.018732 -0.273955   \n",
       "magnesium                            0.236441         0.199950  0.055398   \n",
       "total_phenols                        0.612413        -0.055136  0.433681   \n",
       "flavanoids                           0.652692        -0.172379  0.543479   \n",
       "nonflavanoid_phenols                -0.365845         0.139057 -0.262640   \n",
       "proanthocyanins                      1.000000        -0.025250  0.295544   \n",
       "color_intensity                     -0.025250         1.000000 -0.521813   \n",
       "hue                                  0.295544        -0.521813  1.000000   \n",
       "od280/od315_of_diluted_wines         0.519067        -0.428815  0.565468   \n",
       "proline                              0.330417         0.316100  0.236183   \n",
       "target                              -0.499130         0.265668 -0.617369   \n",
       "\n",
       "                              od280/od315_of_diluted_wines   proline    target  \n",
       "alcohol                                           0.072343  0.643720 -0.328222  \n",
       "malic_acid                                       -0.368710 -0.192011  0.437776  \n",
       "ash                                               0.003911  0.223626 -0.049643  \n",
       "alcalinity_of_ash                                -0.276769 -0.440597  0.517859  \n",
       "magnesium                                         0.066004  0.393351 -0.209179  \n",
       "total_phenols                                     0.699949  0.498115 -0.719163  \n",
       "flavanoids                                        0.787194  0.494193 -0.847498  \n",
       "nonflavanoid_phenols                             -0.503270 -0.311385  0.489109  \n",
       "proanthocyanins                                   0.519067  0.330417 -0.499130  \n",
       "color_intensity                                  -0.428815  0.316100  0.265668  \n",
       "hue                                               0.565468  0.236183 -0.617369  \n",
       "od280/od315_of_diluted_wines                      1.000000  0.312761 -0.788230  \n",
       "proline                                           0.312761  1.000000 -0.633717  \n",
       "target                                           -0.788230 -0.633717  1.000000  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_corr = X.corr()\n",
    "X_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(X_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9). Создайте список high_corr из признаков, корреляция которых с полем target по абсолютному значению превышает 0.5 (причем, само поле target не должно входить в этот список)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcalinity_of_ash               0.517859\n",
       "total_phenols                  -0.719163\n",
       "flavanoids                     -0.847498\n",
       "hue                            -0.617369\n",
       "od280/od315_of_diluted_wines   -0.788230\n",
       "proline                        -0.633717\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_filtered = X_corr.target[abs(X_corr.target) > 0.5].iloc[:-1]\n",
    "corr_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alcalinity_of_ash',\n",
       " 'total_phenols',\n",
       " 'flavanoids',\n",
       " 'hue',\n",
       " 'od280/od315_of_diluted_wines',\n",
       " 'proline']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_corr = list(corr_filtered.keys())\n",
    "high_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10). Удалите из датафрейма X поле с целевой переменной. Для всех признаков, названия которых содержатся в списке high_corr, вычислите квадрат их значений и добавьте в датафрейм X соответствующие поля с суффиксом '_2', добавленного к первоначальному названию признака. Итоговый датафрейм должен содержать все поля, которые, были в нем изначально, а также поля с признаками из списка high_corr, возведенными в квадрат. Выведите описание полей датафрейма X с помощью метода describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  \n",
       "0                            3.92   1065.0  \n",
       "1                            3.40   1050.0  \n",
       "2                            3.17   1185.0  \n",
       "3                            3.45   1480.0  \n",
       "4                            2.93    735.0  \n",
       "..                            ...      ...  \n",
       "173                          1.74    740.0  \n",
       "174                          1.56    750.0  \n",
       "175                          1.56    835.0  \n",
       "176                          1.62    840.0  \n",
       "177                          1.60    560.0  \n",
       "\n",
       "[178 rows x 13 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.drop('target', axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>alcalinity_of_ash_2</th>\n",
       "      <th>total_phenols_2</th>\n",
       "      <th>flavanoids_2</th>\n",
       "      <th>hue_2</th>\n",
       "      <th>od280/od315_of_diluted_wines_2</th>\n",
       "      <th>proline_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>243.36</td>\n",
       "      <td>7.8400</td>\n",
       "      <td>9.3636</td>\n",
       "      <td>1.0816</td>\n",
       "      <td>15.3664</td>\n",
       "      <td>1134225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>125.44</td>\n",
       "      <td>7.0225</td>\n",
       "      <td>7.6176</td>\n",
       "      <td>1.1025</td>\n",
       "      <td>11.5600</td>\n",
       "      <td>1102500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>345.96</td>\n",
       "      <td>7.8400</td>\n",
       "      <td>10.4976</td>\n",
       "      <td>1.0609</td>\n",
       "      <td>10.0489</td>\n",
       "      <td>1404225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>282.24</td>\n",
       "      <td>14.8225</td>\n",
       "      <td>12.1801</td>\n",
       "      <td>0.7396</td>\n",
       "      <td>11.9025</td>\n",
       "      <td>2190400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>441.00</td>\n",
       "      <td>7.8400</td>\n",
       "      <td>7.2361</td>\n",
       "      <td>1.0816</td>\n",
       "      <td>8.5849</td>\n",
       "      <td>540225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>420.25</td>\n",
       "      <td>2.8224</td>\n",
       "      <td>0.3721</td>\n",
       "      <td>0.4096</td>\n",
       "      <td>3.0276</td>\n",
       "      <td>547600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>529.00</td>\n",
       "      <td>3.2400</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.4900</td>\n",
       "      <td>2.4336</td>\n",
       "      <td>562500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>400.00</td>\n",
       "      <td>2.5281</td>\n",
       "      <td>0.4761</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>2.4336</td>\n",
       "      <td>697225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>400.00</td>\n",
       "      <td>2.7225</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>2.6244</td>\n",
       "      <td>705600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>600.25</td>\n",
       "      <td>4.2025</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>0.3721</td>\n",
       "      <td>2.5600</td>\n",
       "      <td>313600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  alcalinity_of_ash_2  \\\n",
       "0                            3.92   1065.0               243.36   \n",
       "1                            3.40   1050.0               125.44   \n",
       "2                            3.17   1185.0               345.96   \n",
       "3                            3.45   1480.0               282.24   \n",
       "4                            2.93    735.0               441.00   \n",
       "..                            ...      ...                  ...   \n",
       "173                          1.74    740.0               420.25   \n",
       "174                          1.56    750.0               529.00   \n",
       "175                          1.56    835.0               400.00   \n",
       "176                          1.62    840.0               400.00   \n",
       "177                          1.60    560.0               600.25   \n",
       "\n",
       "     total_phenols_2  flavanoids_2   hue_2  od280/od315_of_diluted_wines_2  \\\n",
       "0             7.8400        9.3636  1.0816                         15.3664   \n",
       "1             7.0225        7.6176  1.1025                         11.5600   \n",
       "2             7.8400       10.4976  1.0609                         10.0489   \n",
       "3            14.8225       12.1801  0.7396                         11.9025   \n",
       "4             7.8400        7.2361  1.0816                          8.5849   \n",
       "..               ...           ...     ...                             ...   \n",
       "173           2.8224        0.3721  0.4096                          3.0276   \n",
       "174           3.2400        0.5625  0.4900                          2.4336   \n",
       "175           2.5281        0.4761  0.3481                          2.4336   \n",
       "176           2.7225        0.4624  0.3600                          2.6244   \n",
       "177           4.2025        0.5776  0.3721                          2.5600   \n",
       "\n",
       "     proline_2  \n",
       "0    1134225.0  \n",
       "1    1102500.0  \n",
       "2    1404225.0  \n",
       "3    2190400.0  \n",
       "4     540225.0  \n",
       "..         ...  \n",
       "173   547600.0  \n",
       "174   562500.0  \n",
       "175   697225.0  \n",
       "176   705600.0  \n",
       "177   313600.0  \n",
       "\n",
       "[178 rows x 19 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for label in high_corr:\n",
    "    new_label = f'{label}_2'\n",
    "    X[new_label] = X[label]**2\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>alcalinity_of_ash_2</th>\n",
       "      <th>total_phenols_2</th>\n",
       "      <th>flavanoids_2</th>\n",
       "      <th>hue_2</th>\n",
       "      <th>od280/od315_of_diluted_wines_2</th>\n",
       "      <th>proline_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>1.780000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>746.893258</td>\n",
       "      <td>391.142865</td>\n",
       "      <td>5.657030</td>\n",
       "      <td>5.110049</td>\n",
       "      <td>0.968661</td>\n",
       "      <td>7.322155</td>\n",
       "      <td>6.564591e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>314.907474</td>\n",
       "      <td>133.671775</td>\n",
       "      <td>2.936294</td>\n",
       "      <td>4.211441</td>\n",
       "      <td>0.443798</td>\n",
       "      <td>3.584316</td>\n",
       "      <td>5.558591e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>112.360000</td>\n",
       "      <td>0.960400</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>1.612900</td>\n",
       "      <td>7.728400e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>500.500000</td>\n",
       "      <td>295.840000</td>\n",
       "      <td>3.036325</td>\n",
       "      <td>1.452100</td>\n",
       "      <td>0.612325</td>\n",
       "      <td>3.754075</td>\n",
       "      <td>2.505010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>2.135000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>673.500000</td>\n",
       "      <td>380.250000</td>\n",
       "      <td>5.546050</td>\n",
       "      <td>4.558250</td>\n",
       "      <td>0.931250</td>\n",
       "      <td>7.728400</td>\n",
       "      <td>4.536045e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>462.250000</td>\n",
       "      <td>7.840000</td>\n",
       "      <td>8.265700</td>\n",
       "      <td>1.254400</td>\n",
       "      <td>10.048900</td>\n",
       "      <td>9.702250e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1680.000000</td>\n",
       "      <td>900.000000</td>\n",
       "      <td>15.054400</td>\n",
       "      <td>25.806400</td>\n",
       "      <td>2.924100</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.822400e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\n",
       "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
       "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
       "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
       "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
       "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
       "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
       "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
       "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
       "\n",
       "       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "count     178.000000  178.000000            178.000000       178.000000   \n",
       "mean        2.295112    2.029270              0.361854         1.590899   \n",
       "std         0.625851    0.998859              0.124453         0.572359   \n",
       "min         0.980000    0.340000              0.130000         0.410000   \n",
       "25%         1.742500    1.205000              0.270000         1.250000   \n",
       "50%         2.355000    2.135000              0.340000         1.555000   \n",
       "75%         2.800000    2.875000              0.437500         1.950000   \n",
       "max         3.880000    5.080000              0.660000         3.580000   \n",
       "\n",
       "       color_intensity         hue  od280/od315_of_diluted_wines      proline  \\\n",
       "count       178.000000  178.000000                    178.000000   178.000000   \n",
       "mean          5.058090    0.957449                      2.611685   746.893258   \n",
       "std           2.318286    0.228572                      0.709990   314.907474   \n",
       "min           1.280000    0.480000                      1.270000   278.000000   \n",
       "25%           3.220000    0.782500                      1.937500   500.500000   \n",
       "50%           4.690000    0.965000                      2.780000   673.500000   \n",
       "75%           6.200000    1.120000                      3.170000   985.000000   \n",
       "max          13.000000    1.710000                      4.000000  1680.000000   \n",
       "\n",
       "       alcalinity_of_ash_2  total_phenols_2  flavanoids_2       hue_2  \\\n",
       "count           178.000000       178.000000    178.000000  178.000000   \n",
       "mean            391.142865         5.657030      5.110049    0.968661   \n",
       "std             133.671775         2.936294      4.211441    0.443798   \n",
       "min             112.360000         0.960400      0.115600    0.230400   \n",
       "25%             295.840000         3.036325      1.452100    0.612325   \n",
       "50%             380.250000         5.546050      4.558250    0.931250   \n",
       "75%             462.250000         7.840000      8.265700    1.254400   \n",
       "max             900.000000        15.054400     25.806400    2.924100   \n",
       "\n",
       "       od280/od315_of_diluted_wines_2     proline_2  \n",
       "count                      178.000000  1.780000e+02  \n",
       "mean                         7.322155  6.564591e+05  \n",
       "std                          3.584316  5.558591e+05  \n",
       "min                          1.612900  7.728400e+04  \n",
       "25%                          3.754075  2.505010e+05  \n",
       "50%                          7.728400  4.536045e+05  \n",
       "75%                         10.048900  9.702250e+05  \n",
       "max                         16.000000  2.822400e+06  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
